# Environment Variables Configuration for LLM Cybersecurity Benchmark
# Copy this file to .env and fill in your actual values
# The .env file will be automatically loaded by the benchmark system

# ============================================================================
# DIRECTORY PATHS
# ============================================================================

# Base directory for storing results (logs, outputs, reports)
# Default: ./results
RESULTS_DIR=./results

# Directory containing datasets
# Default: ./data
DATA_DIR=./data

# Directory containing local models
# Default: ./models
MODEL_DIR=./models

# Temporary directory for intermediate files
# Default: /tmp (Unix) or system temp directory
TEMP_DIR=/tmp

# ============================================================================
# API KEYS - REQUIRED FOR API MODELS
# ============================================================================

# OpenAI API Configuration
# Required for: gpt-3.5-turbo, gpt-4, and other OpenAI models
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Custom OpenAI API base URL (for proxies or custom deployments)
# OPENAI_API_BASE=https://api.openai.com/v1

# Optional: OpenAI Organization ID
# OPENAI_ORG_ID=your_org_id_here

# Anthropic API Configuration
# Required for: Claude models (claude-3-sonnet, etc.)
# Get your key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Optional: Custom Anthropic API base URL
# ANTHROPIC_API_BASE=https://api.anthropic.com

# ============================================================================
# DATA SOURCE CREDENTIALS
# ============================================================================

# Kaggle API Credentials
# Required for: Kaggle datasets
# Get credentials from: https://www.kaggle.com/settings/account
# Download kaggle.json and extract username/key
KAGGLE_USERNAME=your_kaggle_username
KAGGLE_KEY=your_kaggle_api_key

# HuggingFace Token
# Required for: Private HuggingFace datasets and some models
# Get token from: https://huggingface.co/settings/tokens
HF_TOKEN=your_huggingface_token

# ============================================================================
# SYSTEM CONFIGURATION
# ============================================================================

# Application Environment
# Options: development, staging, production
# Affects logging levels, caching behavior, and performance optimizations
ENVIRONMENT=development

# Debug Mode
# Set to 'true' to enable verbose debugging output
# WARNING: May expose sensitive information in logs
DEBUG=false

# Logging Level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
# DEBUG: Most verbose, includes all internal operations
# INFO: Standard operational messages (recommended)
# WARNING: Only warnings and errors
# ERROR: Only errors and critical issues
# CRITICAL: Only critical system failures
LOG_LEVEL=INFO

# ============================================================================
# HARDWARE AND PERFORMANCE SETTINGS
# ============================================================================

# MLX Device Configuration (for local Apple Silicon models)
# Options: mps (Apple Silicon GPU), cpu
# mps: Use Apple Silicon GPU acceleration (recommended for M1/M2/M3 Macs)
# cpu: Use CPU only (slower but more compatible)
MLX_DEVICE=mps

# Maximum Memory Usage (in GB)
# Helps prevent out-of-memory errors with large models
# Adjust based on your system's available RAM
MAX_MEMORY_GB=16

# Number of CPU cores to use for parallel processing
# Default: Auto-detect based on system
# Set to lower value if you need to reserve CPU for other tasks
# MAX_CPU_CORES=4

# ============================================================================
# CLOUD STORAGE (OPTIONAL)
# ============================================================================

# AWS S3 Configuration
# Required only if using S3 for data storage or model hosting
# AWS_ACCESS_KEY_ID=your_aws_access_key
# AWS_SECRET_ACCESS_KEY=your_aws_secret_key
# AWS_S3_BUCKET=your_s3_bucket_name
# AWS_REGION=us-east-1

# Google Cloud Storage Configuration
# Required only if using GCS for data storage
# GCP_PROJECT=your_gcp_project_id
# GCP_STORAGE_BUCKET=your_gcs_bucket_name
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json

# ============================================================================
# MONITORING AND LOGGING INTEGRATIONS
# ============================================================================

# Weights & Biases (wandb) Integration
# Optional: For experiment tracking and visualization
# Get API key from: https://wandb.ai/authorize
# WANDB_API_KEY=your_wandb_api_key
# WANDB_ENTITY=your_wandb_username_or_team
# WANDB_PROJECT=llm-cybersec-benchmark

# MLflow Tracking Server
# Optional: For experiment tracking
# MLFLOW_TRACKING_URI=http://localhost:5000
# MLFLOW_EXPERIMENT_NAME=cybersec-benchmark

# ============================================================================
# NOTIFICATION SETTINGS (OPTIONAL)
# ============================================================================

# Slack Notifications
# Optional: Get notified when experiments complete or fail
# Create webhook at: https://api.slack.com/incoming-webhooks
# SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK

# Email Notifications
# Optional: Email notifications for long-running experiments
# SMTP_SERVER=smtp.gmail.com
# SMTP_PORT=587
# SMTP_USERNAME=your_email@gmail.com
# SMTP_PASSWORD=your_app_password
# NOTIFY_EMAIL=recipient@example.com

# ============================================================================
# DATASET-SPECIFIC SETTINGS
# ============================================================================

# Custom Dataset URLs
# Override default dataset locations
# NSL_KDD_URL=https://example.com/custom/nsl-kdd-dataset.csv
# MALWARE_DATASET_URL=https://example.com/malware-data.zip

# Dataset Processing Settings
# MAX_DATASET_SIZE_GB=10     # Skip datasets larger than this
# DATASET_CACHE_DAYS=30      # Days to cache downloaded datasets
# AUTO_DOWNLOAD_DATASETS=true # Automatically download missing datasets

# ============================================================================
# SECURITY SETTINGS
# ============================================================================

# Data Privacy Settings
# ANONYMIZE_LOGS=true        # Remove sensitive info from logs
# ENCRYPT_CACHE=false        # Encrypt cached data (requires setup)
# AUDIT_API_CALLS=true       # Log all API calls for auditing

# SSL/TLS Settings
# VERIFY_SSL_CERTS=true      # Verify SSL certificates (recommended)
# SSL_CERT_PATH=/path/to/custom/cert.pem  # Custom SSL certificate

# ============================================================================
# DEVELOPMENT AND TESTING
# ============================================================================

# Test Mode Settings
# RUN_SMOKE_TESTS=true       # Run quick validation tests
# USE_SAMPLE_DATA=false      # Use small sample datasets for testing
# SKIP_EXPENSIVE_OPS=false   # Skip time-consuming operations in dev mode

# Model Testing
# TEST_MODEL_ENDPOINTS=true  # Test API connectivity before experiments
# VALIDATE_MODEL_OUTPUTS=true # Extra validation of model responses

# ============================================================================
# ADVANCED CONFIGURATION
# ============================================================================

# Rate Limiting
# API_RATE_LIMIT_RPM=60      # Requests per minute limit
# API_RATE_LIMIT_TPM=40000   # Tokens per minute limit

# Retry Configuration
# MAX_API_RETRIES=3          # Maximum API call retries
# RETRY_DELAY_SECONDS=1      # Initial delay between retries
# EXPONENTIAL_BACKOFF=true   # Increase delay exponentially

# Cache Configuration
# ENABLE_RESULT_CACHE=true   # Cache experiment results
# CACHE_EXPIRY_HOURS=24      # Hours until cache expires
# CACHE_COMPRESSION=true     # Compress cached data

# ============================================================================
# USAGE EXAMPLES AND TIPS
# ============================================================================

# Example: Basic setup for OpenAI API usage
# OPENAI_API_KEY=sk-your-key-here
# RESULTS_DIR=./my_experiments
# DATA_DIR=./datasets
# LOG_LEVEL=INFO

# Example: Local model setup with MLX
# MODEL_DIR=./models/mlx
# MLX_DEVICE=mps
# MAX_MEMORY_GB=8

# Example: Full cloud integration
# OPENAI_API_KEY=sk-your-key-here
# KAGGLE_USERNAME=username
# KAGGLE_KEY=your-key
# HF_TOKEN=hf_your-token
# WANDB_API_KEY=your-wandb-key
# SLACK_WEBHOOK_URL=https://hooks.slack.com/services/...

# ============================================================================
# TROUBLESHOOTING
# ============================================================================

# Common Issues:
#
# 1. "API key not found" error:
#    - Check that OPENAI_API_KEY or ANTHROPIC_API_KEY is set correctly
#    - Ensure no extra spaces or quotes around the key
#    - Verify the key is valid and has sufficient credits
#
# 2. "Dataset not found" error:
#    - Check that DATA_DIR exists and is writable
#    - For Kaggle datasets, ensure KAGGLE_USERNAME and KAGGLE_KEY are set
#    - For HuggingFace datasets, set HF_TOKEN if required
#
# 3. "Model loading failed" error:
#    - Check that MODEL_DIR exists and contains the model files
#    - For MLX models, ensure MLX_DEVICE is set correctly for your hardware
#    - Verify sufficient memory with MAX_MEMORY_GB setting
#
# 4. "Permission denied" errors:
#    - Ensure all directory paths are writable
#    - Check file permissions on model and data directories
#    - Run with appropriate user permissions
#
# 5. Performance issues:
#    - Adjust MAX_MEMORY_GB based on available system RAM
#    - Use MLX_DEVICE=mps on Apple Silicon for GPU acceleration
#    - Monitor logs for memory or timeout warnings

# ============================================================================
# SECURITY NOTES
# ============================================================================

# IMPORTANT SECURITY CONSIDERATIONS:
#
# 1. Never commit the actual .env file to version control
# 2. Keep API keys secure and rotate them regularly
# 3. Use environment-specific .env files (.env.dev, .env.prod)
# 4. Consider using a secrets management system for production
# 5. Regularly review and audit API usage
# 6. Use least-privilege principles for cloud credentials
# 7. Monitor for unauthorized API usage
# 8. Enable logging and monitoring for security events
